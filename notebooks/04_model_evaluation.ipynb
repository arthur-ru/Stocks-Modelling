{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69f5a899",
   "metadata": {},
   "source": [
    "\n",
    "# Model Evaluation for Stock Price Prediction\n",
    "\n",
    "This notebook demonstrates the evaluation process for the GRU model used in predicting stock prices. It covers the steps of preparing the test data, evaluating the model's performance, and interpreting the results.\n",
    "\n",
    "1. Importing Libraries and Modules\n",
    "2. Preparing Test Data\n",
    "3. Evaluating Model Performance\n",
    "4. Interpreting Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b637500",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importing Libraries and Modules\n",
    "\n",
    "First, we import necessary libraries and modules required for the evaluation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9367e1",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Preparing Test Data\n",
    "\n",
    "In this step, we prepare the test data to be fed into the model for evaluation. This includes loading the test dataset and creating a DataLoader for it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91bca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model_training import data, model, StockDataset, window_size, device, criterion, training_row_index, index\n",
    "\n",
    "test_stock_data = data.iloc[training_row_index:, index]\n",
    "test_stock_data = test_stock_data.to_numpy()\n",
    "\n",
    "stock_test_dataset = StockDataset(test_stock_data, window_size)\n",
    "test_loader = DataLoader(stock_test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b8821",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Evaluating Model Performance\n",
    "\n",
    "Here, we evaluate the model's performance on the test set. This involves setting the model to evaluation mode and computing the loss for each sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1ab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()        # Set the model to evaluation mode\n",
    "total_loss = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():  # No need to track the gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Resize labels for shape matching\n",
    "        labels = labels.unsqueeze(1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491c60e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Interpreting Results\n",
    "\n",
    "In this final step, we interpret the results obtained from the model evaluation, analyzing the model's predictive capabilities and accuracy.\n",
    "\n",
    "We also change the hyperparameters in order to reduce the average loss. [IN PROGRESS]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
