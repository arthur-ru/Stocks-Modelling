{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df253aa3",
   "metadata": {},
   "source": [
    "\n",
    "# Model Training for Stock Prediction using GRU\n",
    "\n",
    "This notebook is based on the `model_training.py` script and demonstrates the process of setting up and training a GRU (Gated Recurrent Unit) neural network model to predict the evolution of S&P 500 stock prices. It includes details on importing libraries, loading configurations, defining the network architecture, and placeholders for data preparation, model training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c72b0",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Importing Libraries and Configuration\n",
    "\n",
    "The first step involves importing necessary libraries and loading the configuration settings from a YAML file. To install the yaml library, execute the command `pip install pyyaml` in the console of your IDE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the configuration file\n",
    "with open('config/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "batch_size = config['batch_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045108f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Defining the GRU Network\n",
    "\n",
    "The `GRUNet` class is defined to create a GRU neural network. This network includes a GRU layer and a fully connected layer. The network is initialized with input dimensions, hidden dimensions, output dimensions, and the number of layers. The forward method defines how the network processes input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028523eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79917d",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Data Loading and Preparation\n",
    "\n",
    "In this section, we will load the preprocessed data and prepare it for training. This involves defining the dataset structure, normalizing data, and splitting it into training and test sets.\n",
    "\n",
    "### 3.1. Setting file path and loading data into a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/raw_data/data_stock.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Giving ticker symbol for the training model\n",
    "\n",
    "Specify in `input_value` the ticker value for which the model will be trained (e.g., 'MSFT' for Microsoft). \n",
    "Then, define a function to find the given index of the stock ticker in the DataFrame's columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the stock to train by giving the ticker you want in input_value\n",
    "input_value = 'MSFT'\n",
    "\n",
    "def get_index(data, input_value):\n",
    "    for index, value in enumerate(data):\n",
    "        if input_value == value:\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "index = get_index(data.columns, input_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Select the data to train on\n",
    "\n",
    "We use 80% of the data for training and 20% for evaluating the model, that we convert in a numpy array for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_row_index = int((len(data.iloc[:,index]))*0.8)\n",
    "current_stock_data = data.iloc[:training_row_index, index]\n",
    "current_stock_data = current_stock_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b731f",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Model Training\n",
    "\n",
    "Here we cover the process of training the GRU model. This includes defining the loss function, the optimizer, and the training loop where the model learns from the training data over several epochs.\n",
    "\n",
    "### 4.1. Definition of a Dataset\n",
    "\n",
    "For training the GRU model, we need to define a custom dataset. This involves creating a class that inherits from PyTorch's `Dataset` class. The dataset class will be responsible for loading the stock data, processing it, and returning samples in a format suitable for the GRU model.\n",
    "\n",
    "In this section, we will define such a dataset class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \"\"\" Personnalised dataset for stock prices \"\"\"\n",
    "    def __init__(self, data, window_size):\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx:idx + self.window_size]\n",
    "        label = self.data[idx + self.window_size]\n",
    "        return torch.tensor(seq, dtype=torch.float).unsqueeze(-1), torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StockDataset` class is defined to handle stock price data. It includes the following methods:\n",
    "\n",
    "1. `__init__`: Constructor for the class, initializes the dataset with stock price data and a specified window size for creating sequences.\n",
    "2. `__len__`: Returns the total number of samples in the dataset.\n",
    "3. `__getitem__`: Retrieves a single sample from the dataset at a given index. This method returns a sequence of data points for the input and the next data point as the label.\n",
    "\n",
    "You will need to further develop this class according to your specific requirements, such as data normalization, sequence creation, and handling the input and target data for the GRU model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Instanciation of the model\n",
    "\n",
    "#### 4.2.1 Define the window size\n",
    "Here, we use the last 60 days of data to predict the stock price of the next day.  \n",
    "\n",
    "Creating the dataset and DataLoader for training. The DataLoader fetches batches from the StockDataset without shuffling, because order matters in time series.\n",
    "\n",
    "Instanciation of the GRU model with specific dimensions and layers.\n",
    "\n",
    "Defining the loss function (Mean Squared Error Loss) and the optimizer (Adam) for the training.\n",
    "\n",
    "Setting the number of epochs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Create the dataset & DataLoader for training\n",
    "The DataLoader fetches batches from the StockDataset without shuffling, because order matters in time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dataset = StockDataset(current_stock_data, window_size)\n",
    "train_loader = DataLoader(stock_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3. Instantiating the GRU model\n",
    "\n",
    "Specific dimensions and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUNet(input_dim=1, hidden_dim=100, output_dim=1, num_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4. Defining the loss function & the optimizer\n",
    "\n",
    "In our case, we chose the Mean Square Error Loss & the Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.5. Setting the number of epochs\n",
    "\n",
    "Epochs designs the number of complete iteration through a dataset during the training of a model. In our case, we have determined that the optimal number of epochs was 93.\n",
    "\n",
    "This is valid for the following ticker & hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value = \"AAPL\"\n",
    "learning_rate: 0.001\n",
    "batch_size: 32\n",
    "num_epochs: 93\n",
    "hidden_dim: 100\n",
    "num_layers: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.6. Training loop\n",
    "\n",
    "The model is trained over multiple epochs. In each epoch, the model processes batches of data, calculates the loss, and updates its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving the model\n",
    "\n",
    "For the ticker put in `input_value`, we save the trained model state. The name of the `.pth` file allows to retrace which model is done for which action. For now, we have trained models for the `AAPL`, `AMZN` & `MSFT` stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/model_\"+ str(input_value) +\".pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
